# -*- coding: utf-8 -*-
"""NeuralNetworkFinal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gt-KSxugUyqhvbOuvkUdJC0R8YhB1R7V
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn import metrics
import time
import sys
import time

layer_sizes = [784,500,100,25,10]# this is the architecture of the model, no. of neurons in each layer


trainPath = sys.argv[1]# path of training data, expected in csv format with last column as labels
arr = np.loadtxt(trainPath, delimiter=",", dtype=int)

testPath = sys.argv[2]# path of test data, expected in csv format with last column as labels
arr1 = np.loadtxt(testPath, delimiter=",", dtype=int)

Ytrain = arr[:,-1].astype(int)
Xtrain = arr[:,0:-1].astype(int)/255

Ytest = arr1[:,-1].astype(int)
Xtest = arr1[:,0:-1].astype(int)/255

def oneHotEncoding(Ytrain):
    out = []
    for i in Ytrain:
        val = np.zeros(10).astype(int)
        val[i] = 1
        out.append(val)
    return np.array(out)

YtrainNew = oneHotEncoding(Ytrain).T
output = sys.argv[3]# path of the output data, where train accuracy of the model will be stored


# Hlist = hidden layer list

def sigmoid(arr):
    return np.exp(arr)/(1+np.exp(arr))

def reLU(arr):
    ans = []
    for i in arr:
        if i>=0:
            ans.append(i)
        else:
            ans.append(0)
    return np.array(ans)

def initialSetUp(Hlist,X,Y,batch = 1):#Hlist :  Hidden layer list
    WBmatrix = {}
    for i in range(1, len(Hlist)):
        WBmatrix[f'W{i}'] = np.random.randn(Hlist[i], Hlist[i-1])
    return WBmatrix


def feedf(WBmatrix,Xtrain,Hlist, batch,m):
    layers = {}
    outputs = []
    for n in range(len(Hlist)):
        layers[f'postActivation{n}'] = 0
    for k in range(m,m+batch):
        
        for i in range(1,len(Hlist)):
            if i==1:
                layers[f'preActivation{i}'] = WBmatrix[f'W{i}']@Xtrain[k].T# + WBmatrix[f'B{i}']
                layers[f'postActivation{i}'] = sigmoid(layers[f'preActivation{i}'])
            else:
                layers[f'preActivation{i}'] = WBmatrix[f'W{i}']@layers[f'postActivation{i-1}']# + WBmatrix[f'B{i}']
                layers[f'postActivation{i}'] = sigmoid(layers[f'preActivation{i}'])
        outputs.append(layers[f'postActivation{len(Hlist)-1}'])
    return layers, outputs

def lossValue(layers,Ytrain,output,batch,m):
    loss = 0
    output = np.array(output).T
    for k in range(m,m+batch):
        Ypred = output.T[k-m]
        loss = loss + 1/(2*batch) * np.sum(np.square(Ypred - YtrainNew.T[k].reshape(10,-1)))
        # loss = loss + metrics.log_loss(YtrainNew.T[k],Ypred)
    return loss

def jacobian(postactivation):
    l = len(postactivation)
    ans = np.zeros((l,l))
    val = postactivation * (1-postactivation)
    for i in range(l):
        ans[i,i] = val[i]
    return ans

def backwardPropagation(WBmatrix, layers, Xtrain, Ytrain,outputs,Hlist,batch,m):
    depth = len(WBmatrix)#//2
    grads = {}
    outputs = np.array(outputs)
    for i in range(1, depth+1):
        grads[f'W{i}'] = np.zeros((Hlist[i], Hlist[i-1]))

    for k in range(m,batch+m):
        for i in range(depth,0,-1):
            if i==depth:
                dL = outputs.T[:,k-m].reshape(10,1) - YtrainNew.T[k].reshape(10,1)

                dAz = jacobian(sigmoid(layers[f'preActivation{i}']))@dL
            else:
                dAz = dAz @ WBmatrix[f'W{i+1}']
                dAz = dAz @ jacobian(sigmoid(layers[f'preActivation{i}']))
                dAz = dAz.T

            if i==1:
                Xtrain = np.array(Xtrain)
                dAz =np.array(dAz)
                grads[f'W{i}'] = np.add(grads[f'W{i}'] , dAz@(Xtrain[k].reshape(-1,784)))
            else:
                grads[f'W{i}'] = np.add( grads[f'W{i}'], dAz@(layers[f'postActivation{i-1}'].T.reshape(1,-1)))
                dAz = dAz.T
    return grads

def updation(params, grads, eta, batch):
    layers = len(params)
    params_updated = {}
    for i in range(1,layers+1):
        params[f'W{i}'] = params[f'W{i}'] - eta *(1/batch)* grads[f'W{i}']
    return params

def model(Xtrain, Ytrain, Hlist, epochs, eta, batch):
    WBmatrix = initialSetUp(Hlist,Xtrain,Ytrain)
    last = np.shape(Ytrain)[1]
    for i in range(epochs):
        for m in range(0,last,batch):
            layers, outputs = feedf(WBmatrix,Xtrain,Hlist, batch,m)
            cost = lossValue(layers, Ytrain.T,outputs, batch,m)
            grads = backwardPropagation(WBmatrix, layers, Xtrain, Ytrain, outputs,Hlist, batch,m)
            WBmatrix = updation(WBmatrix, grads, eta, batch)
        # eta = eta/np.sqrt(epochs+1)
        print(f'Cost at sample value {m} is {cost}')
        print(f'epoch no. {i+1} is complete')
    return WBmatrix


def compute_accuracy(X_train, X_test, Y_train, Y_test, params,Hlist,batch):
    val, uselessHere = np.array(predict(X_train, params, Hlist,batch))
    acc = 0
    for k in range(batch):
        if (val.T[:,k] == YtrainNew.T[k]).all():
            acc +=1
    return acc/batch

def predict(X, params, Hlist,size):
    ans = []

    values, outputs = feedf(params,X,Hlist,size,0)
    pre = outputs
    for m in range(size):
        predictions = np.zeros(10)
        predictions[np.argmax(pre[m])] = 1
        ans.append(list(predictions))
    
    return ans, pre

def oneHot2labels(X, params, Hlist,size):
    labels = []
    a,Ypred = predict(X, params, Hlist,size)
    for i in range(np.shape(Ypred)[0]):
        labels.append(np.argmax(Ypred[i]))
    return labels

print('Neural network formed')

#*************************************************************************************
if __name__ == "__main__":

    a = time.time()
    params = model(Xtrain, YtrainNew, layer_sizes, 3, .1,100)#Xtrain, Ytrain, Hlist, epochs, learning_rate, batch_size
    b  = time.time()
    print(f'time taken to train is: {b-a} seconds')


    predicted = oneHot2labels(Xtrain, params, layer_sizes,60000)
    # np.shape(Ytrain)

    confusion_matrix = metrics.confusion_matrix(Ytrain, predicted)
    accuracy = metrics.accuracy_score(Ytrain, predicted)

    with open(output, "a") as f:
        print(f'The train accuracy of the model is {accuracy}',file = f)


    cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

    cm_display.plot()
    plt.show()
