# -*- coding: utf-8 -*-
"""NeuralNetworkFinal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gt-KSxugUyqhvbOuvkUdJC0R8YhB1R7V
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn import metrics
import time
import sys
trainPath = sys.argv[1]
arr = np.loadtxt(trainPath, delimiter=",", dtype=int)

testPath = sys.argv[2]
arr1 = np.loadtxt(testPath, delimiter=",", dtype=int)

Ytrain = arr[:,-1].astype(int)
Xtrain = arr[:,0:-1].astype(int)/255

Ytest = arr1[:,-1].astype(int)
Xtest = arr1[:,0:-1].astype(int)/255

def oneHotEncoding(Ytrain):
    out = []
    for i in Ytrain:
        val = np.zeros(10).astype(int)
        val[i] = 1
        out.append(val)
    return np.array(out)

YtrainNew = oneHotEncoding(Ytrain).T
output = sys.argv[3]
part = sys.argv[4]

"""A"""

# Hlist = hidden layer list
if part == 'a':
  def sigmoid(arr):
      return np.exp(arr)/(1+np.exp(arr))

  def reLU(arr):
      ans = []
      for i in arr:
          if i>=0:
              ans.append(i)
          else:
              ans.append(0)
      return np.array(ans)

  def initialSetUp(Hlist,X,Y,batch = 1):#Hlist :  Hidden layer list
      WBmatrix = {}
      for i in range(1, len(Hlist)):
          WBmatrix[f'W{i}'] = np.random.randn(Hlist[i], Hlist[i-1])
      return WBmatrix


  def feedf(WBmatrix,Xtrain,Hlist, batch,m):
      layers = {}
      outputs = []
      for n in range(len(Hlist)):
          layers[f'postActivation{n}'] = 0
      for k in range(m,m+batch):
          
          for i in range(1,len(Hlist)):
              if i==1:
                  layers[f'preActivation{i}'] = WBmatrix[f'W{i}']@Xtrain[k].T# + WBmatrix[f'B{i}']
                  layers[f'postActivation{i}'] = sigmoid(layers[f'preActivation{i}'])
              else:
                  layers[f'preActivation{i}'] = WBmatrix[f'W{i}']@layers[f'postActivation{i-1}']# + WBmatrix[f'B{i}']
                  layers[f'postActivation{i}'] = sigmoid(layers[f'preActivation{i}'])
          outputs.append(layers[f'postActivation{len(Hlist)-1}'])
      return layers, outputs

  def lossValue(layers,Ytrain,output,batch,m):
      loss = 0
      output = np.array(output).T
      for k in range(m,m+batch):
          Ypred = output.T[k-m]
          loss = loss + 1/(2*batch) * np.sum(np.square(Ypred - YtrainNew.T[k].reshape(10,-1)))
          # loss = loss + metrics.log_loss(YtrainNew.T[k],Ypred)
      return loss

  def jacobian(postactivation):
      l = len(postactivation)
      ans = np.zeros((l,l))
      val = postactivation * (1-postactivation)
      for i in range(l):
          ans[i,i] = val[i]
      return ans

  def backwardPropagation(WBmatrix, layers, Xtrain, Ytrain,outputs,Hlist,batch,m):
      depth = len(WBmatrix)#//2
      grads = {}
      outputs = np.array(outputs)
      for i in range(1, depth+1):
          grads[f'W{i}'] = np.zeros((Hlist[i], Hlist[i-1]))

      for k in range(m,batch+m):
          for i in range(depth,0,-1):
              if i==depth:
                  dL = outputs.T[:,k-m].reshape(10,1) - YtrainNew.T[k].reshape(10,1)

                  dAz = jacobian(sigmoid(layers[f'preActivation{i}']))@dL
              else:
                  dAz = dAz @ WBmatrix[f'W{i+1}']
                  dAz = dAz @ jacobian(sigmoid(layers[f'preActivation{i}']))
                  dAz = dAz.T

              if i==1:
                  Xtrain = np.array(Xtrain)
                  dAz =np.array(dAz)
                  grads[f'W{i}'] = np.add(grads[f'W{i}'] , dAz@(Xtrain[k].reshape(-1,784)))
              else:
                  grads[f'W{i}'] = np.add( grads[f'W{i}'], dAz@(layers[f'postActivation{i-1}'].T.reshape(1,-1)))
                  dAz = dAz.T
      return grads

  def updation(params, grads, eta, batch):
      layers = len(params)
      params_updated = {}
      for i in range(1,layers+1):
          params[f'W{i}'] = params[f'W{i}'] - eta *(1/batch)* grads[f'W{i}']
      return params

  def model(Xtrain, Ytrain, Hlist, epochs, eta, batch):
      WBmatrix = initialSetUp(Hlist,Xtrain,Ytrain)
      last = np.shape(Ytrain)[1]
      for i in range(epochs):
          for m in range(0,last,batch):
              layers, outputs = feedf(WBmatrix,Xtrain,Hlist, batch,m)
              cost = lossValue(layers, Ytrain.T,outputs, batch,m)
              grads = backwardPropagation(WBmatrix, layers, Xtrain, Ytrain, outputs,Hlist, batch,m)
              WBmatrix = updation(WBmatrix, grads, eta, batch)
          # eta = eta/np.sqrt(epochs+1)
          print(f'Cost at sample value {m} is {cost}')
          print(f'epoch no. {i+1} is complete')
      return WBmatrix


  def compute_accuracy(X_train, X_test, Y_train, Y_test, params,Hlist,batch):
      val, uselessHere = np.array(predict(X_train, params, Hlist,batch))
      acc = 0
      for k in range(batch):
          if (val.T[:,k] == YtrainNew.T[k]).all():
              acc +=1
      return acc/batch

  def predict(X, params, Hlist,size):
      ans = []

      values, outputs = feedf(params,X,Hlist,size,0)
      pre = outputs
      for m in range(size):
          predictions = np.zeros(10)
          predictions[np.argmax(pre[m])] = 1
          ans.append(list(predictions))
      
      return ans, pre

  def oneHot2labels(X, params, Hlist,size):
      labels = []
      a,Ypred = predict(X, params, Hlist,size)
      for i in range(np.shape(Ypred)[0]):
          labels.append(np.argmax(Ypred[i]))
      return labels

  print('Neural network formed')

"""B"""

if part =='b':
  # Hlist = hidden layer list

  def sigmoid(arr):
      return np.exp(arr)/(1+np.exp(arr))

  def reLU(arr):
      ans = []
      for i in arr:
          if i>=0:
              ans.append(i)
          else:
              ans.append(0)
      return np.array(ans)

  def initialSetUp(Hlist,X,Y,batch = 1):#Hlist :  Hidden layer list
      WBmatrix = {}
      for i in range(1, len(Hlist)):
          WBmatrix[f'W{i}'] = np.random.randn(Hlist[i], Hlist[i-1])
      return WBmatrix


  def feedf(WBmatrix,Xtrain,Hlist, batch,m):
      layers = {}
      outputs = []
      for n in range(len(Hlist)):
          layers[f'postActivation{n}'] = 0
      for k in range(m,m+batch):
          
          for i in range(1,len(Hlist)):
              if i==1:
                  layers[f'preActivation{i}'] = WBmatrix[f'W{i}']@Xtrain[k].T# + WBmatrix[f'B{i}']
                  layers[f'postActivation{i}'] = sigmoid(layers[f'preActivation{i}'])
              else:
                  layers[f'preActivation{i}'] = WBmatrix[f'W{i}']@layers[f'postActivation{i-1}']# + WBmatrix[f'B{i}']
                  layers[f'postActivation{i}'] = sigmoid(layers[f'preActivation{i}'])
          outputs.append(layers[f'postActivation{len(Hlist)-1}'])
      return layers, outputs

  def lossValue(layers,Ytrain,output,batch,m):
      loss = 0
      output = np.array(output).T
      for k in range(m,m+batch):
          Ypred = output.T[k-m]
          loss = loss + 1/(2*batch) * np.sum(np.square(Ypred - YtrainNew.T[k].reshape(10,-1)))
          # loss = loss + metrics.log_loss(YtrainNew.T[k],Ypred)
      return loss

  def jacobian(postactivation):
      l = len(postactivation)
      ans = np.zeros((l,l))
      val = postactivation * (1-postactivation)
      for i in range(l):
          ans[i,i] = val[i]
      return ans

  def backwardPropagation(WBmatrix, layers, Xtrain, Ytrain,outputs,Hlist,batch,m):
      depth = len(WBmatrix)#//2
      grads = {}
      outputs = np.array(outputs)
      for i in range(1, depth+1):
          grads[f'W{i}'] = np.zeros((Hlist[i], Hlist[i-1]))

      for k in range(m,batch+m):
          for i in range(depth,0,-1):
              if i==depth:
                  dL = outputs.T[:,k-m].reshape(10,1) - YtrainNew.T[k].reshape(10,1)

                  dAz = jacobian(sigmoid(layers[f'preActivation{i}']))@dL
              else:
                  dAz = dAz @ WBmatrix[f'W{i+1}']
                  dAz = dAz @ jacobian(sigmoid(layers[f'preActivation{i}']))
                  dAz = dAz.T

              if i==1:
                  Xtrain = np.array(Xtrain)
                  dAz =np.array(dAz)
                  grads[f'W{i}'] = np.add(grads[f'W{i}'] , dAz@(Xtrain[k].reshape(-1,784)))
              else:
                  grads[f'W{i}'] = np.add( grads[f'W{i}'], dAz@(layers[f'postActivation{i-1}'].T.reshape(1,-1)))
                  dAz = dAz.T
      return grads

  def updation(params, grads, eta, batch):
      layers = len(params)
      params_updated = {}
      for i in range(1,layers+1):
          params[f'W{i}'] = params[f'W{i}'] - eta *(1/batch)* grads[f'W{i}']
      return params

  def model(Xtrain, Ytrain, Hlist, epochs, eta, batch):
      WBmatrix = initialSetUp(Hlist,Xtrain,Ytrain)
      last = np.shape(Ytrain)[1]
      for i in range(epochs):
          for m in range(0,last,batch):
              layers, outputs = feedf(WBmatrix,Xtrain,Hlist, batch,m)
              cost = lossValue(layers, Ytrain.T,outputs, batch,m)
              grads = backwardPropagation(WBmatrix, layers, Xtrain, Ytrain, outputs,Hlist, batch,m)
              WBmatrix = updation(WBmatrix, grads, eta, batch)
          # eta = eta/np.sqrt(epochs+1)
          print(f'Cost at sample value {m} is {cost}')
          print(f'epoch no. {i+1} is complete')
      return WBmatrix


  def compute_accuracy(X_train, X_test, Y_train, Y_test, params,Hlist,batch):
      val, uselessHere = np.array(predict(X_train, params, Hlist,batch))
      acc = 0
      for k in range(batch):
          if (val.T[:,k] == YtrainNew.T[k]).all():
              acc +=1
      return acc/batch

  def predict(X, params, Hlist,size):
      ans = []

      values, outputs = feedf(params,X,Hlist,size,0)
      pre = outputs
      for m in range(size):
          predictions = np.zeros(10)
          predictions[np.argmax(pre[m])] = 1
          ans.append(list(predictions))
      
      return ans, pre

  def oneHot2labels(X, params, Hlist,size):
      labels = []
      a,Ypred = predict(X, params, Hlist,size)
      for i in range(np.shape(Ypred)[0]):
          labels.append(np.argmax(Ypred[i]))
      return labels



  #*************************************************************************************




  layer_sizes = [784,5,10]
  import time

  a = time.time()
  params = model(Xtrain, YtrainNew, layer_sizes, 3, .1,100)
  b  = time.time()
  print(f'time taken to train is: {b-a} seconds')


  predicted = oneHot2labels(Xtrain, params, layer_sizes,60000)
  # np.shape(Ytrain)

  confusion_matrix = metrics.confusion_matrix(Ytrain, predicted)
  accuracy = metrics.accuracy_score(Ytrain, predicted)

  with open(output, "a") as f:
    print(f'The train accuracy of the model is {accuracy}',file = f)


  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

  cm_display.plot()
  plt.show()



  predicted = oneHot2labels(Xtest, params, layer_sizes,10000)
  np.shape(Ytrain)

  confusion_matrix = metrics.confusion_matrix(Ytest, predicted)
  # accuracy = metrics.accuracy_score(Ytrain, predicted)
  accuracy = metrics.accuracy_score(Ytest, predicted)
  with open(output, "a") as f:
    print(f'The test accuracy of the model is {accuracy}',file = f)

  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

  cm_display.plot()
  plt.show()



  #*****************************************************************************

  layer_sizes = [784,10,10]
  # print(np.shape(YtrainNew))
  # print(np.shape(batc(YtrainNew.T,100)))

  a = time.time()
  params = model(Xtrain, YtrainNew, layer_sizes, 3, .1,100)
  b  = time.time()
  print(f'time taken to train is: {b-a} seconds')


  predicted = oneHot2labels(Xtrain, params, layer_sizes,60000)
  # np.shape(Ytrain)

  confusion_matrix = metrics.confusion_matrix(Ytrain, predicted)
  accuracy = metrics.accuracy_score(Ytrain, predicted)

  with open(output, "a") as f:
    print(f'The train accuracy of the model is {accuracy}',file = f)


  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

  cm_display.plot()
  plt.show()



  predicted = oneHot2labels(Xtest, params, layer_sizes,10000)
  np.shape(Ytrain)

  confusion_matrix = metrics.confusion_matrix(Ytest, predicted)
  # accuracy = metrics.accuracy_score(Ytrain, predicted)
  accuracy = metrics.accuracy_score(Ytest, predicted)
  with open(output, "a") as f:
    print(f'The test accuracy of the model is {accuracy}',file = f)

  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

  cm_display.plot()
  plt.show()

  #********************************************************************************

  layer_sizes = [784,15,10]
  # print(np.shape(YtrainNew))
  # print(np.shape(batc(YtrainNew.T,100)))
 
  a = time.time()
  params = model(Xtrain, YtrainNew, layer_sizes, 3, .1,100)
  b  = time.time()
  print(f'time taken to train is: {b-a} seconds')


  predicted = oneHot2labels(Xtrain, params, layer_sizes,60000)
  # np.shape(Ytrain)

  confusion_matrix = metrics.confusion_matrix(Ytrain, predicted)
  accuracy = metrics.accuracy_score(Ytrain, predicted)

  with open(output, "a") as f:
    print(f'The train accuracy of the model is {accuracy}',file = f)


  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

  cm_display.plot()
  plt.show()



  predicted = oneHot2labels(Xtest, params, layer_sizes,10000)
  np.shape(Ytrain)

  confusion_matrix = metrics.confusion_matrix(Ytest, predicted)
  # accuracy = metrics.accuracy_score(Ytrain, predicted)
  accuracy = metrics.accuracy_score(Ytest, predicted)
  with open(output, "a") as f:
    print(f'The test accuracy of the model is {accuracy}',file = f)

  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

  cm_display.plot()
  plt.show()


  #*******************************************************************************

  layer_sizes = [784,20,10]
  # print(np.shape(YtrainNew))
  # print(np.shape(batc(YtrainNew.T,100)))

  a = time.time()
  params = model(Xtrain, YtrainNew, layer_sizes, 3, .1,100)
  b  = time.time()
  print(f'time taken to train is: {b-a} seconds')


  predicted = oneHot2labels(Xtrain, params, layer_sizes,60000)
  # np.shape(Ytrain)

  confusion_matrix = metrics.confusion_matrix(Ytrain, predicted)
  accuracy = metrics.accuracy_score(Ytrain, predicted)

  with open(output, "a") as f:
    print(f'The train accuracy of the model is {accuracy}',file = f)


  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

  cm_display.plot()
  plt.show()



  predicted = oneHot2labels(Xtest, params, layer_sizes,10000)
  np.shape(Ytrain)

  confusion_matrix = metrics.confusion_matrix(Ytest, predicted)
  # accuracy = metrics.accuracy_score(Ytrain, predicted)
  accuracy = metrics.accuracy_score(Ytest, predicted)
  with open(output, "a") as f:
    print(f'The test accuracy of the model is {accuracy}',file = f)

  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

  cm_display.plot()
  plt.show()


  #******************************************************************************

  layer_sizes = [784,25,10]
  # print(np.shape(YtrainNew))
  # print(np.shape(batc(YtrainNew.T,100)))

  a = time.time()
  params = model(Xtrain, YtrainNew, layer_sizes, 3, .1,100)
  b  = time.time()
  print(f'time taken to train is: {b-a} seconds')


  predicted = oneHot2labels(Xtrain, params, layer_sizes,60000)
  # np.shape(Ytrain)

  confusion_matrix = metrics.confusion_matrix(Ytrain, predicted)
  accuracy = metrics.accuracy_score(Ytrain, predicted)

  with open(output, "a") as f:
    print(f'The train accuracy of the model is {accuracy}',file = f)


  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

  cm_display.plot()
  plt.show()



  predicted = oneHot2labels(Xtest, params, layer_sizes,10000)
  np.shape(Ytrain)

  confusion_matrix = metrics.confusion_matrix(Ytest, predicted)
  # accuracy = metrics.accuracy_score(Ytrain, predicted)
  accuracy = metrics.accuracy_score(Ytest, predicted)
  with open(output, "a") as f:
    print(f'The test accuracy of the model is {accuracy}',file = f)

  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

  cm_display.plot()
  plt.show()

"""C"""

# Hlist = hidden layer list
if part =='c':
  def sigmoid(arr):
      return np.exp(arr)/(1+np.exp(arr))

  def reLU(arr):
      ans = []
      for i in arr:
          if i>=0:
              ans.append(i)
          else:
              ans.append(0)
      return np.array(ans)

  def initialSetUp(Hlist,X,Y,batch = 1):#Hlist :  Hidden layer list
      WBmatrix = {}
      for i in range(1, len(Hlist)):
          WBmatrix[f'W{i}'] = np.random.randn(Hlist[i], Hlist[i-1])
      return WBmatrix


  def feedf(WBmatrix,Xtrain,Hlist, batch,m):
      layers = {}
      outputs = []
      for n in range(len(Hlist)):
          layers[f'postActivation{n}'] = 0
      for k in range(m,m+batch):
          
          for i in range(1,len(Hlist)):
              if i==1:
                  layers[f'preActivation{i}'] = WBmatrix[f'W{i}']@Xtrain[k].T# + WBmatrix[f'B{i}']
                  layers[f'postActivation{i}'] = sigmoid(layers[f'preActivation{i}'])
              else:
                  layers[f'preActivation{i}'] = WBmatrix[f'W{i}']@layers[f'postActivation{i-1}']# + WBmatrix[f'B{i}']
                  layers[f'postActivation{i}'] = sigmoid(layers[f'preActivation{i}'])
          outputs.append(layers[f'postActivation{len(Hlist)-1}'])
      return layers, outputs

  def lossValue(layers,Ytrain,output,batch,m):
      loss = 0
      output = np.array(output).T
      for k in range(m,m+batch):
          Ypred = output.T[k-m]
          loss = loss + 1/(2*batch) * np.sum(np.square(Ypred - YtrainNew.T[k].reshape(10,-1)))
          # loss = loss + metrics.log_loss(YtrainNew.T[k],Ypred)
      return loss

  def jacobian(postactivation):
      l = len(postactivation)
      ans = np.zeros((l,l))
      val = postactivation * (1-postactivation)
      for i in range(l):
          ans[i,i] = val[i]
      return ans

  def backwardPropagation(WBmatrix, layers, Xtrain, Ytrain,outputs,Hlist,batch,m):
      depth = len(WBmatrix)#//2
      grads = {}
      outputs = np.array(outputs)
      for i in range(1, depth+1):
          grads[f'W{i}'] = np.zeros((Hlist[i], Hlist[i-1]))

      for k in range(m,batch+m):
          for i in range(depth,0,-1):
              if i==depth:
                  dL = outputs.T[:,k-m].reshape(10,1) - YtrainNew.T[k].reshape(10,1)

                  dAz = jacobian(sigmoid(layers[f'preActivation{i}']))@dL
              else:
                  dAz = dAz @ WBmatrix[f'W{i+1}']
                  dAz = dAz @ jacobian(sigmoid(layers[f'preActivation{i}']))
                  dAz = dAz.T

              if i==1:
                  Xtrain = np.array(Xtrain)
                  dAz =np.array(dAz)
                  grads[f'W{i}'] = np.add(grads[f'W{i}'] , dAz@(Xtrain[k].reshape(-1,784)))
              else:
                  grads[f'W{i}'] = np.add( grads[f'W{i}'], dAz@(layers[f'postActivation{i-1}'].T.reshape(1,-1)))
                  dAz = dAz.T
      return grads

  def updation(params, grads, eta, batch):
      layers = len(params)
      params_updated = {}
      for i in range(1,layers+1):
          params[f'W{i}'] = params[f'W{i}'] - eta *(1/batch)* grads[f'W{i}']
      return params

  def model(Xtrain, Ytrain, Hlist, epochs, eta, batch):
      WBmatrix = initialSetUp(Hlist,Xtrain,Ytrain)
      last = np.shape(Ytrain)[1]
      for i in range(epochs):
          for m in range(0,last,batch):
              layers, outputs = feedf(WBmatrix,Xtrain,Hlist, batch,m)
              cost = lossValue(layers, Ytrain.T,outputs, batch,m)
              grads = backwardPropagation(WBmatrix, layers, Xtrain, Ytrain, outputs,Hlist, batch,m)
              WBmatrix = updation(WBmatrix, grads, eta, batch)
          eta = eta/np.sqrt(epochs+1)
          print(f'Cost at sample value {m} is {cost}')
          print(f'epoch no. {i+1} is complete')
      return WBmatrix


  def compute_accuracy(X_train, X_test, Y_train, Y_test, params,Hlist,batch):
      val, uselessHere = np.array(predict(X_train, params, Hlist,batch))
      acc = 0
      for k in range(batch):
          if (val.T[:,k] == YtrainNew.T[k]).all():
              acc +=1
      return acc/batch

  def predict(X, params, Hlist,size):
      ans = []

      values, outputs = feedf(params,X,Hlist,size,0)
      pre = outputs
      for m in range(size):
          predictions = np.zeros(10)
          predictions[np.argmax(pre[m])] = 1
          ans.append(list(predictions))
      
      return ans, pre

  def oneHot2labels(X, params, Hlist,size):
      labels = []
      a,Ypred = predict(X, params, Hlist,size)
      for i in range(np.shape(Ypred)[0]):
          labels.append(np.argmax(Ypred[i]))
      return labels

  #*********************************************************************


  layer_sizes = [784,5,10]
  import time

  a = time.time()
  params = model(Xtrain, YtrainNew, layer_sizes, 3, .1,100)
  b  = time.time()
  print(f'time taken to train is: {b-a} seconds')


  predicted = oneHot2labels(Xtrain, params, layer_sizes,60000)
  # np.shape(Ytrain)

  confusion_matrix = metrics.confusion_matrix(Ytrain, predicted)
  accuracy = metrics.accuracy_score(Ytrain, predicted)

  with open(output, "a") as f:
    print(f'The train accuracy of the model is {accuracy}',file = f)


  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

  cm_display.plot()
  plt.show()



  predicted = oneHot2labels(Xtest, params, layer_sizes,10000)
  # np.shape(Ytrain)

  confusion_matrix = metrics.confusion_matrix(Ytest, predicted)
  # accuracy = metrics.accuracy_score(Ytrain, predicted)
  accuracy = metrics.accuracy_score(Ytest, predicted)
  with open(output, "a") as f:
    print(f'The test accuracy of the model is {accuracy}',file = f)

  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

  cm_display.plot()
  plt.show()



  #*****************************************************************************

  layer_sizes = [784,10,10]
  # print(np.shape(YtrainNew))
  # print(np.shape(batc(YtrainNew.T,100)))

  a = time.time()
  params = model(Xtrain, YtrainNew, layer_sizes, 3, .1,100)
  b  = time.time()
  print(f'time taken to train is: {b-a} seconds')


  predicted = oneHot2labels(Xtrain, params, layer_sizes,60000)
  # np.shape(Ytrain)

  confusion_matrix = metrics.confusion_matrix(Ytrain, predicted)
  accuracy = metrics.accuracy_score(Ytrain, predicted)

  with open(output, "a") as f:
    print(f'The train accuracy of the model is {accuracy}',file = f)


  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

  cm_display.plot()
  plt.show()



  predicted = oneHot2labels(Xtest, params, layer_sizes,10000)
  np.shape(Ytrain)

  confusion_matrix = metrics.confusion_matrix(Ytest, predicted)
  # accuracy = metrics.accuracy_score(Ytrain, predicted)
  accuracy = metrics.accuracy_score(Ytest, predicted)
  with open(output, "a") as f:
    print(f'The test accuracy of the model is {accuracy}',file = f)

  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

  cm_display.plot()
  plt.show()

  #********************************************************************************

  layer_sizes = [784,15,10]
  # print(np.shape(YtrainNew))
  # print(np.shape(batc(YtrainNew.T,100)))
 
  a = time.time()
  params = model(Xtrain, YtrainNew, layer_sizes, 3, .1,100)
  b  = time.time()
  print(f'time taken to train is: {b-a} seconds')


  predicted = oneHot2labels(Xtrain, params, layer_sizes,60000)
  # np.shape(Ytrain)

  confusion_matrix = metrics.confusion_matrix(Ytrain, predicted)
  accuracy = metrics.accuracy_score(Ytrain, predicted)

  with open(output, "a") as f:
    print(f'The train accuracy of the model is {accuracy}',file = f)


  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

  cm_display.plot()
  plt.show()



  predicted = oneHot2labels(Xtest, params, layer_sizes,10000)
  np.shape(Ytrain)

  confusion_matrix = metrics.confusion_matrix(Ytest, predicted)
  # accuracy = metrics.accuracy_score(Ytrain, predicted)
  accuracy = metrics.accuracy_score(Ytest, predicted)
  with open(output, "a") as f:
    print(f'The test accuracy of the model is {accuracy}',file = f)

  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

  cm_display.plot()
  plt.show()


  #*******************************************************************************

  layer_sizes = [784,20,10]
  # print(np.shape(YtrainNew))
  # print(np.shape(batc(YtrainNew.T,100)))

  a = time.time()
  params = model(Xtrain, YtrainNew, layer_sizes, 3, .1,100)
  b  = time.time()
  print(f'time taken to train is: {b-a} seconds')


  predicted = oneHot2labels(Xtrain, params, layer_sizes,60000)
  # np.shape(Ytrain)

  confusion_matrix = metrics.confusion_matrix(Ytrain, predicted)
  accuracy = metrics.accuracy_score(Ytrain, predicted)

  with open(output, "a") as f:
    print(f'The train accuracy of the model is {accuracy}',file = f)


  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

  cm_display.plot()
  plt.show()



  predicted = oneHot2labels(Xtest, params, layer_sizes,10000)
  np.shape(Ytrain)

  confusion_matrix = metrics.confusion_matrix(Ytest, predicted)
  # accuracy = metrics.accuracy_score(Ytrain, predicted)
  accuracy = metrics.accuracy_score(Ytest, predicted)
  with open(output, "a") as f:
    print(f'The test accuracy of the model is {accuracy}',file = f)

  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

  cm_display.plot()
  plt.show()


  #******************************************************************************

  layer_sizes = [784,25,10]
  # print(np.shape(YtrainNew))
  # print(np.shape(batc(YtrainNew.T,100)))

  a = time.time()
  params = model(Xtrain, YtrainNew, layer_sizes, 3, .1,100)
  b  = time.time()
  print(f'time taken to train is: {b-a} seconds')


  predicted = oneHot2labels(Xtrain, params, layer_sizes,60000)
  # np.shape(Ytrain)

  confusion_matrix = metrics.confusion_matrix(Ytrain, predicted)
  accuracy = metrics.accuracy_score(Ytrain, predicted)

  with open(output, "a") as f:
    print(f'The train accuracy of the model is {accuracy}',file = f)


  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

  cm_display.plot()
  plt.show()



  predicted = oneHot2labels(Xtest, params, layer_sizes,10000)
  np.shape(Ytrain)

  confusion_matrix = metrics.confusion_matrix(Ytest, predicted)
  # accuracy = metrics.accuracy_score(Ytrain, predicted)
  accuracy = metrics.accuracy_score(Ytest, predicted)
  with open(output, "a") as f:
    print(f'The test accuracy of the model is {accuracy}',file = f)

  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

  cm_display.plot()
  plt.show()

"""D"""

# Hlist = hidden layer list
if part == 'd':
  def sigmoid(arr):
      return np.exp(arr)/(1+np.exp(arr))

  def reLU(arr):
      ans = []
      for i in arr:
          if i>=0:
              ans.append(i)
          else:
              ans.append(0)
      return np.array(ans)

  def initialSetUp(Hlist,X,Y,batch = 1):#Hlist :  Hidden layer list
      WBmatrix = {}
      for i in range(1, len(Hlist)):
          WBmatrix[f'W{i}'] = np.random.normal(0,1/100,(Hlist[i], Hlist[i-1]))
      return WBmatrix


  def feedf(WBmatrix,Xtrain,Hlist, batch,m):
      layers = {}
      outputs = []
      for n in range(len(Hlist)):
          layers[f'postActivation{n}'] = 0
      for k in range(m,m+batch):
          
          for i in range(1,len(Hlist)):
              if i==1:
                  layers[f'preActivation{i}'] = WBmatrix[f'W{i}']@Xtrain[k].T# + WBmatrix[f'B{i}']
                  layers[f'postActivation{i}'] = reLU(layers[f'preActivation{i}'])
              elif i!=len(Hlist)-1:
                  layers[f'preActivation{i}'] = WBmatrix[f'W{i}']@layers[f'postActivation{i-1}']# + WBmatrix[f'B{i}']
                  layers[f'postActivation{i}'] = reLU(layers[f'preActivation{i}'])
              else:
                  layers[f'preActivation{i}'] = WBmatrix[f'W{i}']@layers[f'postActivation{i-1}']# + WBmatrix[f'B{i}']
                  layers[f'postActivation{i}'] = sigmoid(layers[f'preActivation{i}'])
          outputs.append(layers[f'postActivation{len(Hlist)-1}'])
      return layers, outputs

  def lossValue(layers,Ytrain,output,batch,m):
      loss = 0
      output = np.array(output).T
      for k in range(m,m+batch):
          Ypred = output.T[k-m]
          loss = loss + 1/(2*batch) * np.sum(np.square(Ypred - YtrainNew.T[k].reshape(10,-1)))
      return loss

  def jacobians(postactivation):
      l = len(postactivation)
      ans = np.zeros((l,l))
      val = postactivation * (1-postactivation)
      for i in range(l):
          ans[i,i] = val[i]
      return ans


  def jacobianr(postactivation):
      val = []
      l = len(postactivation)
      ans = np.zeros((l,l))
      for i in postactivation:
          if i>0:
              val.append(1)
          else:
              val.append(0)
      for i in range(l):
          ans[i,i] = val[i]
      return np.array(ans)

  def backwardPropagation(WBmatrix, layers, Xtrain, Ytrain,outputs,Hlist,batch,m):
      depth = len(WBmatrix)#//2
      grads = {}
      outputs = np.array(outputs)
      for i in range(1, depth+1):
          grads[f'W{i}'] = np.zeros((Hlist[i], Hlist[i-1]))

      for k in range(m,batch+m):
          for i in range(depth,0,-1):
              if i==depth:
                  dL = outputs.T[:,k-m].reshape(10,1) - YtrainNew.T[k].reshape(10,1)

                  dAz = jacobians(sigmoid(layers[f'preActivation{i}']))@dL
              else:
                  dAz = dAz @ WBmatrix[f'W{i+1}']
                  dAz = dAz @ jacobianr(reLU(layers[f'preActivation{i}']))
                  dAz = dAz.T

              if i==1:
                  Xtrain = np.array(Xtrain)
                  dAz =np.array(dAz)
  #                 print(k)
                  grads[f'W{i}'] = np.add(grads[f'W{i}'] , dAz@(Xtrain[k].reshape(-1,784)))
      #             grads[f'B{i}'] = dAz
              else:
                  grads[f'W{i}'] = np.add( grads[f'W{i}'], dAz@(layers[f'postActivation{i-1}'].T.reshape(1,-1)))
                  dAz = dAz.T
      return grads

  def updation(params, grads, eta, batch):
      layers = len(params)
      params_updated = {}
      for i in range(1,layers+1):
          params[f'W{i}'] = params[f'W{i}'] - eta *(1/batch)* grads[f'W{i}']
      return params

  def model(Xtrain, Ytrain, Hlist, epochs, eta, batch):
      WBmatrix = initialSetUp(Hlist,Xtrain,Ytrain)
      last = np.shape(Ytrain)[1]
      for i in range(epochs):
          for m in range(0,last,batch):
              layers, outputs = feedf(WBmatrix,Xtrain,Hlist, batch,m)
              cost = lossValue(layers, Ytrain.T,outputs, batch,m)
              grads = backwardPropagation(WBmatrix, layers, Xtrain, Ytrain, outputs,Hlist, batch,m)
              WBmatrix = updation(WBmatrix, grads, eta, batch)
              if(m%10 ==0):
                  print('Cost at sample value ' + str(m) + ' = ' + str(cost) + '\n')
                # print(f'epoch no. {i+1} is complete')
          # print('Cost at sample value ' + str(m) + ' = ' + str(cost) + '\n')
      return WBmatrix


  def compute_accuracy(X_train, X_test, Y_train, Y_test, params,Hlist,batch):
      val, uselessHere = np.array(predict(X_train, params, Hlist,batch))
  #     print(np.shape(val))
      # print(np.shape(uselessHere))
  #     print(val.T)
      acc = 0
      for k in range(batch):
  #         comparison = val.T[:,k].reshape(10,1) == YtrainNew.T[k].reshape(10,1)
          if (val.T[:,k] == YtrainNew.T[k]).all():
              acc +=1
  #             print(k)
      return acc/batch

  def predict(X, params, Hlist,size):
      ans = []

      values, outputs = feedf(params,X,Hlist,size,0)
      pre = outputs
      for m in range(size):
          predictions = np.zeros(10)
          predictions[np.argmax(pre[m])] = 1
          ans.append(list(predictions))
      
      return ans, pre

  def oneHot2labels(X, params, Hlist,size):
      labels = []
      a,Ypred = predict(X, params, Hlist,size)
      for i in range(np.shape(Ypred)[0]):
          labels.append(np.argmax(Ypred[i]))
      return labels



  layer_sizes = [784,100,100,10]
  # print(np.shape(YtrainNew))
  # print(np.shape(batc(YtrainNew.T,100)))
  import time

  a = time.time()
  params = model(Xtrain, YtrainNew, layer_sizes, 3, .1,100)
  b  = time.time()
  print(f'time taken to train is: {b-a} seconds')


  predicted = oneHot2labels(Xtrain, params, layer_sizes,60000)
  # np.shape(Ytrain)

  confusion_matrix = metrics.confusion_matrix(Ytrain, predicted)
  accuracy = metrics.accuracy_score(Ytrain, predicted)

  with open(output, "a") as f:
    print(f'The train accuracy of the model is {accuracy}',file = f)


  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

  cm_display.plot()
  plt.show()



  predicted = oneHot2labels(Xtest, params, layer_sizes,10000)
  np.shape(Ytrain)

  confusion_matrix = metrics.confusion_matrix(Ytest, predicted)
  # accuracy = metrics.accuracy_score(Ytrain, predicted)
  accuracy = metrics.accuracy_score(Ytest, predicted)
  with open(output, "a") as f:
    print(f'The test accuracy of the model is {accuracy}',file = f)

  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

  cm_display.plot()
  plt.show()


  # Hlist = hidden layer list

  def sigmoid(arr):
      return np.exp(arr)/(1+np.exp(arr))

  def reLU(arr):
      ans = []
      for i in arr:
          if i>=0:
              ans.append(i)
          else:
              ans.append(0)
      return np.array(ans)

  def initialSetUp(Hlist,X,Y,batch = 1):#Hlist :  Hidden layer list
      WBmatrix = {}
      for i in range(1, len(Hlist)):
          WBmatrix[f'W{i}'] = np.random.randn(Hlist[i], Hlist[i-1])
      return WBmatrix


  def feedf(WBmatrix,Xtrain,Hlist, batch,m):
      layers = {}
      outputs = []
      for n in range(len(Hlist)):
          layers[f'postActivation{n}'] = 0
      for k in range(m,m+batch):
          
          for i in range(1,len(Hlist)):
              if i==1:
                  layers[f'preActivation{i}'] = WBmatrix[f'W{i}']@Xtrain[k].T# + WBmatrix[f'B{i}']
                  layers[f'postActivation{i}'] = sigmoid(layers[f'preActivation{i}'])
              else:
                  layers[f'preActivation{i}'] = WBmatrix[f'W{i}']@layers[f'postActivation{i-1}']# + WBmatrix[f'B{i}']
                  layers[f'postActivation{i}'] = sigmoid(layers[f'preActivation{i}'])
          outputs.append(layers[f'postActivation{len(Hlist)-1}'])
      return layers, outputs

  def lossValue(layers,Ytrain,output,batch,m):
      loss = 0
      output = np.array(output).T
      for k in range(m,m+batch):
          Ypred = output.T[k-m]
          loss = loss + 1/(2*batch) * np.sum(np.square(Ypred - YtrainNew.T[k].reshape(10,-1)))
          # loss = loss + metrics.log_loss(YtrainNew.T[k],Ypred)
      return loss

  def jacobian(postactivation):
      l = len(postactivation)
      ans = np.zeros((l,l))
      val = postactivation * (1-postactivation)
      for i in range(l):
          ans[i,i] = val[i]
      return ans

  def backwardPropagation(WBmatrix, layers, Xtrain, Ytrain,outputs,Hlist,batch,m):
      depth = len(WBmatrix)#//2
      grads = {}
      outputs = np.array(outputs)
      for i in range(1, depth+1):
          grads[f'W{i}'] = np.zeros((Hlist[i], Hlist[i-1]))

      for k in range(m,batch+m):
          for i in range(depth,0,-1):
              if i==depth:
                  dL = outputs.T[:,k-m].reshape(10,1) - YtrainNew.T[k].reshape(10,1)

                  dAz = jacobian(sigmoid(layers[f'preActivation{i}']))@dL
              else:
                  dAz = dAz @ WBmatrix[f'W{i+1}']
                  dAz = dAz @ jacobian(sigmoid(layers[f'preActivation{i}']))
                  dAz = dAz.T

              if i==1:
                  Xtrain = np.array(Xtrain)
                  dAz =np.array(dAz)
                  grads[f'W{i}'] = np.add(grads[f'W{i}'] , dAz@(Xtrain[k].reshape(-1,784)))
              else:
                  grads[f'W{i}'] = np.add( grads[f'W{i}'], dAz@(layers[f'postActivation{i-1}'].T.reshape(1,-1)))
                  dAz = dAz.T
      return grads

  def updation(params, grads, eta, batch):
      layers = len(params)
      params_updated = {}
      for i in range(1,layers+1):
          params[f'W{i}'] = params[f'W{i}'] - eta *(1/batch)* grads[f'W{i}']
      return params

  def model(Xtrain, Ytrain, Hlist, epochs, eta, batch):
      WBmatrix = initialSetUp(Hlist,Xtrain,Ytrain)
      last = np.shape(Ytrain)[1]
      for i in range(epochs):
          for m in range(0,last,batch):
              layers, outputs = feedf(WBmatrix,Xtrain,Hlist, batch,m)
              cost = lossValue(layers, Ytrain.T,outputs, batch,m)
              grads = backwardPropagation(WBmatrix, layers, Xtrain, Ytrain, outputs,Hlist, batch,m)
              WBmatrix = updation(WBmatrix, grads, eta, batch)
          # eta = eta/np.sqrt(epochs+1)
          print(f'Cost at sample value {m} is {cost}')
          print(f'epoch no. {i+1} is complete')
      return WBmatrix


  def compute_accuracy(X_train, X_test, Y_train, Y_test, params,Hlist,batch):
      val, uselessHere = np.array(predict(X_train, params, Hlist,batch))
      acc = 0
      for k in range(batch):
          if (val.T[:,k] == YtrainNew.T[k]).all():
              acc +=1
      return acc/batch

  def predict(X, params, Hlist,size):
      ans = []

      values, outputs = feedf(params,X,Hlist,size,0)
      pre = outputs
      for m in range(size):
          predictions = np.zeros(10)
          predictions[np.argmax(pre[m])] = 1
          ans.append(list(predictions))
      
      return ans, pre

  def oneHot2labels(X, params, Hlist,size):
      labels = []
      a,Ypred = predict(X, params, Hlist,size)
      for i in range(np.shape(Ypred)[0]):
          labels.append(np.argmax(Ypred[i]))
      return labels


  layer_sizes = [784,100,100,10]
  # print(np.shape(YtrainNew))
  # print(np.shape(batc(YtrainNew.T,100)))
  import time

  a = time.time()
  params = model(Xtrain, YtrainNew, layer_sizes, 3, .1,100)
  b  = time.time()
  print(f'time taken to train is: {b-a} seconds')


  predicted = oneHot2labels(Xtrain, params, layer_sizes,60000)
  # np.shape(Ytrain)

  confusion_matrix = metrics.confusion_matrix(Ytrain, predicted)
  accuracy = metrics.accuracy_score(Ytrain, predicted)

  with open(output, "a") as f:
    print(f'The train accuracy of the model is {accuracy}',file = f)


  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

  cm_display.plot()
  plt.show()



  predicted = oneHot2labels(Xtest, params, layer_sizes,10000)
  np.shape(Ytrain)

  confusion_matrix = metrics.confusion_matrix(Ytest, predicted)
  # accuracy = metrics.accuracy_score(Ytrain, predicted)
  accuracy = metrics.accuracy_score(Ytest, predicted)
  with open(output, "a") as f:
    print(f'The test accuracy of the model is {accuracy}',file = f)

  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

  cm_display.plot()
  plt.show()

"""E"""

# Hlist = hidden layer list
if part =='e':
  def sigmoid(arr):
      return np.exp(arr)/(1+np.exp(arr))

  def reLU(arr):
      ans = []
      for i in arr:
          if i>=0:
              ans.append(i)
          else:
              ans.append(0)
      return np.array(ans)

  def initialSetUp(Hlist,X,Y,batch = 1):#Hlist :  Hidden layer list
      WBmatrix = {}
      for i in range(1, len(Hlist)):
          WBmatrix[f'W{i}'] = np.random.normal(0,1/100,(Hlist[i], Hlist[i-1]))
      return WBmatrix


  def feedf(WBmatrix,Xtrain,Hlist, batch,m):
      layers = {}
      outputs = []
      for n in range(len(Hlist)):
          layers[f'postActivation{n}'] = 0
      for k in range(m,m+batch):
          
          for i in range(1,len(Hlist)):
              if i==1:
                  layers[f'preActivation{i}'] = WBmatrix[f'W{i}']@Xtrain[k].T# + WBmatrix[f'B{i}']
                  layers[f'postActivation{i}'] = reLU(layers[f'preActivation{i}'])
              elif i!=len(Hlist)-1:
                  layers[f'preActivation{i}'] = WBmatrix[f'W{i}']@layers[f'postActivation{i-1}']# + WBmatrix[f'B{i}']
                  layers[f'postActivation{i}'] = reLU(layers[f'preActivation{i}'])
              else:
                  layers[f'preActivation{i}'] = WBmatrix[f'W{i}']@layers[f'postActivation{i-1}']# + WBmatrix[f'B{i}']
                  layers[f'postActivation{i}'] = sigmoid(layers[f'preActivation{i}'])
          outputs.append(layers[f'postActivation{len(Hlist)-1}'])
      return layers, outputs

  def lossValue(layers,Ytrain,output,batch,m):
      loss = 0
      output = np.array(output).T
      for k in range(m,m+batch):
          Ypred = output.T[k-m]
          loss = loss + 1/(2*batch) * np.sum(np.square(Ypred - YtrainNew.T[k].reshape(10,-1)))
      return loss

  def jacobians(postactivation):
      l = len(postactivation)
      ans = np.zeros((l,l))
      val = postactivation * (1-postactivation)
      for i in range(l):
          ans[i,i] = val[i]
      return ans


  def jacobianr(postactivation):
      val = []
      l = len(postactivation)
      ans = np.zeros((l,l))
      for i in postactivation:
          if i>0:
              val.append(1)
          else:
              val.append(0)
      for i in range(l):
          ans[i,i] = val[i]
      return np.array(ans)

  def backwardPropagation(WBmatrix, layers, Xtrain, Ytrain,outputs,Hlist,batch,m):
      depth = len(WBmatrix)#//2
      grads = {}
      outputs = np.array(outputs)
      for i in range(1, depth+1):
          grads[f'W{i}'] = np.zeros((Hlist[i], Hlist[i-1]))

      for k in range(m,batch+m):
          for i in range(depth,0,-1):
              if i==depth:
                  dL = outputs.T[:,k-m].reshape(10,1) - YtrainNew.T[k].reshape(10,1)

                  dAz = jacobians(sigmoid(layers[f'preActivation{i}']))@dL
              else:
                  dAz = dAz @ WBmatrix[f'W{i+1}']
                  dAz = dAz @ jacobianr(reLU(layers[f'preActivation{i}']))
                  dAz = dAz.T

              if i==1:
                  Xtrain = np.array(Xtrain)
                  dAz =np.array(dAz)
  #                 print(k)
                  grads[f'W{i}'] = np.add(grads[f'W{i}'] , dAz@(Xtrain[k].reshape(-1,784)))
      #             grads[f'B{i}'] = dAz
              else:
                  grads[f'W{i}'] = np.add( grads[f'W{i}'], dAz@(layers[f'postActivation{i-1}'].T.reshape(1,-1)))
                  dAz = dAz.T
      return grads

  def updation(params, grads, eta, batch):
      layers = len(params)
      params_updated = {}
      for i in range(1,layers+1):
          params[f'W{i}'] = params[f'W{i}'] - eta *(1/batch)* grads[f'W{i}']
      return params

  def model(Xtrain, Ytrain, Hlist, epochs, eta, batch):
      WBmatrix = initialSetUp(Hlist,Xtrain,Ytrain)
      last = np.shape(Ytrain)[1]
      for i in range(epochs):
          for m in range(0,last,batch):
              layers, outputs = feedf(WBmatrix,Xtrain,Hlist, batch,m)
              cost = lossValue(layers, Ytrain.T,outputs, batch,m)
              grads = backwardPropagation(WBmatrix, layers, Xtrain, Ytrain, outputs,Hlist, batch,m)
              WBmatrix = updation(WBmatrix, grads, eta, batch)
              if(m%10 ==0):
                  print('Cost at sample value ' + str(m) + ' = ' + str(cost) + '\n')
                # print(f'epoch no. {i+1} is complete')
          # print('Cost at sample value ' + str(m) + ' = ' + str(cost) + '\n')
      return WBmatrix


  def compute_accuracy(X_train, X_test, Y_train, Y_test, params,Hlist,batch):
      val, uselessHere = np.array(predict(X_train, params, Hlist,batch))
  #     print(np.shape(val))
      print(np.shape(uselessHere))
  #     print(val.T)
      acc = 0
      for k in range(batch):
  #         comparison = val.T[:,k].reshape(10,1) == YtrainNew.T[k].reshape(10,1)
          if (val.T[:,k] == YtrainNew.T[k]).all():
              acc +=1
  #             print(k)
      return acc/batch

  def predict(X, params, Hlist,size):
      ans = []

      values, outputs = feedf(params,X,Hlist,size,0)
      pre = outputs
      for m in range(size):
          predictions = np.zeros(10)
          predictions[np.argmax(pre[m])] = 1
          ans.append(list(predictions))
      
      return ans, pre

  def oneHot2labels(X, params, Hlist,size):
      labels = []
      a,Ypred = predict(X, params, Hlist,size)
      for i in range(np.shape(Ypred)[0]):
          labels.append(np.argmax(Ypred[i]))
      return labels


  # Computation starts from here
  layer_sizes = [784,50,50,10]
  # print(np.shape(YtrainNew))
  # print(np.shape(batc(YtrainNew.T,100)))
  import time

  a = time.time()
  params = model(Xtrain, YtrainNew, layer_sizes, 4, .6,1)
  b  = time.time()
  print(f'time taken to train is: {b-a} seconds')


  predicted = oneHot2labels(Xtrain, params, layer_sizes,60000)
  # np.shape(Ytrain)

  confusion_matrix = metrics.confusion_matrix(Ytrain, predicted)
  accuracy = metrics.accuracy_score(Ytrain, predicted)

  with open(output, "a") as f:
    print(f'The train accuracy of the model is {accuracy}',file = f)


  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

  cm_display.plot()
  plt.show()



  predicted = oneHot2labels(Xtest, params, layer_sizes,10000)
  np.shape(Ytrain)

  confusion_matrix = metrics.confusion_matrix(Ytest, predicted)
  # accuracy = metrics.accuracy_score(Ytrain, predicted)
  accuracy = metrics.accuracy_score(Ytest, predicted)
  with open(output, "a") as f:
    print(f'The test accuracy of the model is {accuracy}',file = f)

  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

  cm_display.plot()
  plt.show()

  #****************************************************************************


  layer_sizes = [784,50,50,50,10]
  # print(np.shape(YtrainNew))
  # print(np.shape(batc(YtrainNew.T,100)))
  import time

  a = time.time()
  params = model(Xtrain, YtrainNew, layer_sizes, 4, .6,1)
  b  = time.time()
  print(f'time taken to train is: {b-a} seconds')


  predicted = oneHot2labels(Xtrain, params, layer_sizes,60000)
  # np.shape(Ytrain)

  confusion_matrix = metrics.confusion_matrix(Ytrain, predicted)
  accuracy = metrics.accuracy_score(Ytrain, predicted)

  with open(output, "a") as f:
    print(f'The train accuracy of the model is {accuracy}',file = f)


  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

  cm_display.plot()
  plt.show()



  predicted = oneHot2labels(Xtest, params, layer_sizes,10000)
  np.shape(Ytrain)

  confusion_matrix = metrics.confusion_matrix(Ytest, predicted)
  # accuracy = metrics.accuracy_score(Ytrain, predicted)
  accuracy = metrics.accuracy_score(Ytest, predicted)
  with open(output, "a") as f:
    print(f'The test accuracy of the model is {accuracy}',file = f)

  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

  cm_display.plot()
  plt.show()

  #********************************************************************************

  layer_sizes = [784,50,50,50,50,10]
  # print(np.shape(YtrainNew))
  # print(np.shape(batc(YtrainNew.T,100)))
  import time

  a = time.time()
  params = model(Xtrain, YtrainNew, layer_sizes, 4, .6,1)
  b  = time.time()
  print(f'time taken to train is: {b-a} seconds')



  predicted = oneHot2labels(Xtrain, params, layer_sizes,60000)
  # np.shape(Ytrain)

  confusion_matrix = metrics.confusion_matrix(Ytrain, predicted)
  accuracy = metrics.accuracy_score(Ytrain, predicted)

  with open(output, "a") as f:
    print(f'The train accuracy of the model is {accuracy}',file = f)


  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

  cm_display.plot()
  plt.show()



  predicted = oneHot2labels(Xtest, params, layer_sizes,10000)
  np.shape(Ytrain)

  confusion_matrix = metrics.confusion_matrix(Ytest, predicted)
  # accuracy = metrics.accuracy_score(Ytrain, predicted)
  accuracy = metrics.accuracy_score(Ytest, predicted)
  with open(output, "a") as f:
    print(f'The test accuracy of the model is {accuracy}',file = f)

  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

  cm_display.plot()
  plt.show()


  #****************************************************************************

  layer_sizes = [784,50,50,50,50,50,10]
  # print(np.shape(YtrainNew))
  # print(np.shape(batc(YtrainNew.T,100)))
  import time

  a = time.time()
  params = model(Xtrain, YtrainNew, layer_sizes, 3, .6,1)
  b  = time.time()
  print(f'time taken to train is: {b-a} seconds')


  predicted = oneHot2labels(Xtrain, params, layer_sizes,60000)
  # np.shape(Ytrain)

  confusion_matrix = metrics.confusion_matrix(Ytrain, predicted)
  accuracy = metrics.accuracy_score(Ytrain, predicted)

  with open(output, "a") as f:
    print(f'The train accuracy of the model is {accuracy}',file = f)


  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

  cm_display.plot()
  plt.show()



  predicted = oneHot2labels(Xtest, params, layer_sizes,10000)
  np.shape(Ytrain)

  confusion_matrix = metrics.confusion_matrix(Ytest, predicted)
  # accuracy = metrics.accuracy_score(Ytrain, predicted)
  accuracy = metrics.accuracy_score(Ytest, predicted)
  with open(output, "a") as f:
    print(f'The test accuracy of the model is {accuracy}',file = f)

  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

  cm_display.plot()
  plt.show()

  # Hlist = hidden layer list

  def sigmoid(arr):
      return np.exp(arr)/(1+np.exp(arr))

  def reLU(arr):
      ans = []
      for i in arr:
          if i>=0:
              ans.append(i)
          else:
              ans.append(0)
      return np.array(ans)

  def initialSetUp(Hlist,X,Y,batch = 1):#Hlist :  Hidden layer list
      WBmatrix = {}
      for i in range(1, len(Hlist)):
          WBmatrix[f'W{i}'] = np.random.randn(Hlist[i], Hlist[i-1])
      return WBmatrix


  def feedf(WBmatrix,Xtrain,Hlist, batch,m):
      layers = {}
      outputs = []
      for n in range(len(Hlist)):
          layers[f'postActivation{n}'] = 0
      for k in range(m,m+batch):
          
          for i in range(1,len(Hlist)):
              if i==1:
                  layers[f'preActivation{i}'] = WBmatrix[f'W{i}']@Xtrain[k].T# + WBmatrix[f'B{i}']
                  layers[f'postActivation{i}'] = sigmoid(layers[f'preActivation{i}'])
              else:
                  layers[f'preActivation{i}'] = WBmatrix[f'W{i}']@layers[f'postActivation{i-1}']# + WBmatrix[f'B{i}']
                  layers[f'postActivation{i}'] = sigmoid(layers[f'preActivation{i}'])
          outputs.append(layers[f'postActivation{len(Hlist)-1}'])
      return layers, outputs

  def lossValue(layers,Ytrain,output,batch,m):
      loss = 0
      output = np.array(output).T
      for k in range(m,m+batch):
          Ypred = output.T[k-m]
          loss = loss + 1/(2*batch) * np.sum(np.square(Ypred - YtrainNew.T[k].reshape(10,-1)))
          # loss = loss + metrics.log_loss(YtrainNew.T[k],Ypred)
      return loss

  def jacobian(postactivation):
      l = len(postactivation)
      ans = np.zeros((l,l))
      val = postactivation * (1-postactivation)
      for i in range(l):
          ans[i,i] = val[i]
      return ans

  def backwardPropagation(WBmatrix, layers, Xtrain, Ytrain,outputs,Hlist,batch,m):
      depth = len(WBmatrix)#//2
      grads = {}
      outputs = np.array(outputs)
      for i in range(1, depth+1):
          grads[f'W{i}'] = np.zeros((Hlist[i], Hlist[i-1]))

      for k in range(m,batch+m):
          for i in range(depth,0,-1):
              if i==depth:
                  dL = outputs.T[:,k-m].reshape(10,1) - YtrainNew.T[k].reshape(10,1)

                  dAz = jacobian(sigmoid(layers[f'preActivation{i}']))@dL
              else:
                  dAz = dAz @ WBmatrix[f'W{i+1}']
                  dAz = dAz @ jacobian(sigmoid(layers[f'preActivation{i}']))
                  dAz = dAz.T

              if i==1:
                  Xtrain = np.array(Xtrain)
                  dAz =np.array(dAz)
                  grads[f'W{i}'] = np.add(grads[f'W{i}'] , dAz@(Xtrain[k].reshape(-1,784)))
              else:
                  grads[f'W{i}'] = np.add( grads[f'W{i}'], dAz@(layers[f'postActivation{i-1}'].T.reshape(1,-1)))
                  dAz = dAz.T
      return grads

  def updation(params, grads, eta, batch):
      layers = len(params)
      params_updated = {}
      for i in range(1,layers+1):
          params[f'W{i}'] = params[f'W{i}'] - eta *(1/batch)* grads[f'W{i}']
      return params

  def model(Xtrain, Ytrain, Hlist, epochs, eta, batch):
      WBmatrix = initialSetUp(Hlist,Xtrain,Ytrain)
      last = np.shape(Ytrain)[1]
      for i in range(epochs):
          for m in range(0,last,batch):
              layers, outputs = feedf(WBmatrix,Xtrain,Hlist, batch,m)
              cost = lossValue(layers, Ytrain.T,outputs, batch,m)
              grads = backwardPropagation(WBmatrix, layers, Xtrain, Ytrain, outputs,Hlist, batch,m)
              WBmatrix = updation(WBmatrix, grads, eta, batch)
          # eta = eta/np.sqrt(epochs+1)
          print(f'Cost at sample value {m} is {cost}')
          print(f'epoch no. {i+1} is complete')
      return WBmatrix


  def compute_accuracy(X_train, X_test, Y_train, Y_test, params,Hlist,batch):
      val, uselessHere = np.array(predict(X_train, params, Hlist,batch))
      acc = 0
      for k in range(batch):
          if (val.T[:,k] == YtrainNew.T[k]).all():
              acc +=1
      return acc/batch

  def predict(X, params, Hlist,size):
      ans = []

      values, outputs = feedf(params,X,Hlist,size,0)
      pre = outputs
      for m in range(size):
          predictions = np.zeros(10)
          predictions[np.argmax(pre[m])] = 1
          ans.append(list(predictions))
      
      return ans, pre

  def oneHot2labels(X, params, Hlist,size):
      labels = []
      a,Ypred = predict(X, params, Hlist,size)
      for i in range(np.shape(Ypred)[0]):
          labels.append(np.argmax(Ypred[i]))
      return labels


  # Computation starts from here
  layer_sizes = [784,50,50,10]
  # print(np.shape(YtrainNew))
  # print(np.shape(batc(YtrainNew.T,100)))
  import time

  a = time.time()
  params = model(Xtrain, YtrainNew, layer_sizes, 4, .6,1)
  b  = time.time()
  print(f'time taken to train is: {b-a} seconds')



  predicted = oneHot2labels(Xtrain, params, layer_sizes,60000)
  # np.shape(Ytrain)

  confusion_matrix = metrics.confusion_matrix(Ytrain, predicted)
  accuracy = metrics.accuracy_score(Ytrain, predicted)

  with open(output, "a") as f:
    print(f'The train accuracy of the model is {accuracy}',file = f)


  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

  cm_display.plot()
  plt.show()



  predicted = oneHot2labels(Xtest, params, layer_sizes,10000)
  np.shape(Ytrain)

  confusion_matrix = metrics.confusion_matrix(Ytest, predicted)
  # accuracy = metrics.accuracy_score(Ytrain, predicted)
  accuracy = metrics.accuracy_score(Ytest, predicted)
  with open(output, "a") as f:
    print(f'The test accuracy of the model is {accuracy}',file = f)

  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

  cm_display.plot()
  plt.show()

  #****************************************************************************


  layer_sizes = [784,50,50,50,10]
  # print(np.shape(YtrainNew))
  # print(np.shape(batc(YtrainNew.T,100)))
  import time

  a = time.time()
  params = model(Xtrain, YtrainNew, layer_sizes, 4, .6,1)
  b  = time.time()
  print(f'time taken to train is: {b-a} seconds')


  predicted = oneHot2labels(Xtrain, params, layer_sizes,60000)
  # np.shape(Ytrain)

  confusion_matrix = metrics.confusion_matrix(Ytrain, predicted)
  accuracy = metrics.accuracy_score(Ytrain, predicted)

  with open(output, "a") as f:
    print(f'The train accuracy of the model is {accuracy}',file = f)


  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

  cm_display.plot()
  plt.show()



  predicted = oneHot2labels(Xtest, params, layer_sizes,10000)
  np.shape(Ytrain)

  confusion_matrix = metrics.confusion_matrix(Ytest, predicted)
  # accuracy = metrics.accuracy_score(Ytrain, predicted)
  accuracy = metrics.accuracy_score(Ytest, predicted)
  with open(output, "a") as f:
    print(f'The test accuracy of the model is {accuracy}',file = f)

  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

  cm_display.plot()
  plt.show()

  #********************************************************************************

  layer_sizes = [784,50,50,50,50,10]
  # print(np.shape(YtrainNew))
  # print(np.shape(batc(YtrainNew.T,100)))
  import time

  a = time.time()
  params = model(Xtrain, YtrainNew, layer_sizes, 4, .6,1)
  b  = time.time()
  print(f'time taken to train is: {b-a} seconds')





  predicted = oneHot2labels(Xtrain, params, layer_sizes,60000)
  # np.shape(Ytrain)

  confusion_matrix = metrics.confusion_matrix(Ytrain, predicted)
  accuracy = metrics.accuracy_score(Ytrain, predicted)

  with open(output, "a") as f:
    print(f'The train accuracy of the model is {accuracy}',file = f)


  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

  cm_display.plot()
  plt.show()



  predicted = oneHot2labels(Xtest, params, layer_sizes,10000)
  np.shape(Ytrain)

  confusion_matrix = metrics.confusion_matrix(Ytest, predicted)
  # accuracy = metrics.accuracy_score(Ytrain, predicted)
  accuracy = metrics.accuracy_score(Ytest, predicted)
  with open(output, "a") as f:
    print(f'The test accuracy of the model is {accuracy}',file = f)

  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

  cm_display.plot()
  plt.show()

  #****************************************************************************

  layer_sizes = [784,50,50,50,50,50,10]
  # print(np.shape(YtrainNew))
  # print(np.shape(batc(YtrainNew.T,100)))
  import time

  a = time.time()
  params = model(Xtrain, YtrainNew, layer_sizes, 3, .6,1)
  b  = time.time()
  print(f'time taken to train is: {b-a} seconds')



  predicted = oneHot2labels(Xtrain, params, layer_sizes,60000)
  # np.shape(Ytrain)

  confusion_matrix = metrics.confusion_matrix(Ytrain, predicted)
  accuracy = metrics.accuracy_score(Ytrain, predicted)

  with open(output, "a") as f:
    print(f'The train accuracy of the model is {accuracy}',file = f)


  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

  cm_display.plot()
  plt.show()



  predicted = oneHot2labels(Xtest, params, layer_sizes,10000)
  np.shape(Ytrain)

  confusion_matrix = metrics.confusion_matrix(Ytest, predicted)
  # accuracy = metrics.accuracy_score(Ytrain, predicted)
  accuracy = metrics.accuracy_score(Ytest, predicted)
  with open(output, "a") as f:
    print(f'The test accuracy of the model is {accuracy}',file = f)

  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

  cm_display.plot()
  plt.show()

"""F"""

# Hlist = hidden layer list
if part =='f':
  def sigmoid(arr):
      return np.exp(arr)/(1+np.exp(arr))

  def reLU(arr):
      ans = []
      for i in arr:
          if i>=0:
              ans.append(i)
          else:
              ans.append(0)
      return np.array(ans)

  def initialSetUp(Hlist,X,Y,batch = 1):#Hlist :  Hidden layer list
      WBmatrix = {}
      for i in range(1, len(Hlist)):
          WBmatrix[f'W{i}'] = np.random.randn(Hlist[i], Hlist[i-1])
      return WBmatrix


  def feedf(WBmatrix,Xtrain,Hlist, batch,m):
      layers = {}
      outputs = []
      for n in range(len(Hlist)):
          layers[f'postActivation{n}'] = 0
      for k in range(m,m+batch):
          
          for i in range(1,len(Hlist)):
              if i==1:
                  layers[f'preActivation{i}'] = WBmatrix[f'W{i}']@Xtrain[k].T# + WBmatrix[f'B{i}']
                  layers[f'postActivation{i}'] = sigmoid(layers[f'preActivation{i}'])
              else:
                  layers[f'preActivation{i}'] = WBmatrix[f'W{i}']@layers[f'postActivation{i-1}']# + WBmatrix[f'B{i}']
                  layers[f'postActivation{i}'] = sigmoid(layers[f'preActivation{i}'])
          outputs.append(layers[f'postActivation{len(Hlist)-1}'])
      return layers, outputs

  def lossValue(layers,Ytrain,output,batch,m):
      loss = 0
      output = np.array(output).T
      for k in range(m,m+batch):
          Ypred = output.T[k-m]
          # loss = loss + 1/(2*batch) * np.sum(np.square(Ypred - YtrainNew.T[k].reshape(10,-1)))
          loss = loss + metrics.log_loss(YtrainNew.T[k],Ypred)
      return loss

  def jacobian(postactivation):
      l = len(postactivation)
      ans = np.zeros((l,l))
      val = postactivation * (1-postactivation)
      for i in range(l):
          ans[i,i] = val[i]
      return ans

  def backwardPropagation(WBmatrix, layers, Xtrain, Ytrain,outputs,Hlist,batch,m):
      depth = len(WBmatrix)#//2
      grads = {}
      outputs = np.array(outputs)
      for i in range(1, depth+1):
          grads[f'W{i}'] = np.zeros((Hlist[i], Hlist[i-1]))

      for k in range(m,batch+m):
          for i in range(depth,0,-1):
              if i==depth:
                  dL = outputs.T[:,k-m].reshape(10,1) - YtrainNew.T[k].reshape(10,1)

                  dAz = jacobian(sigmoid(layers[f'preActivation{i}']))@dL
              else:
                  dAz = dAz @ WBmatrix[f'W{i+1}']
                  dAz = dAz @ jacobian(sigmoid(layers[f'preActivation{i}']))
                  dAz = dAz.T

              if i==1:
                  Xtrain = np.array(Xtrain)
                  dAz =np.array(dAz)
                  grads[f'W{i}'] = np.add(grads[f'W{i}'] , dAz@(Xtrain[k].reshape(-1,784)))
              else:
                  grads[f'W{i}'] = np.add( grads[f'W{i}'], dAz@(layers[f'postActivation{i-1}'].T.reshape(1,-1)))
                  dAz = dAz.T
      return grads

  def updation(params, grads, eta, batch):
      layers = len(params)
      params_updated = {}
      for i in range(1,layers+1):
          params[f'W{i}'] = params[f'W{i}'] - eta *(1/batch)* grads[f'W{i}']
      return params

  def model(Xtrain, Ytrain, Hlist, epochs, eta, batch):
      WBmatrix = initialSetUp(Hlist,Xtrain,Ytrain)
      last = np.shape(Ytrain)[1]
      for i in range(epochs):
          for m in range(0,last,batch):
              layers, outputs = feedf(WBmatrix,Xtrain,Hlist, batch,m)
              cost = lossValue(layers, Ytrain.T,outputs, batch,m)
              grads = backwardPropagation(WBmatrix, layers, Xtrain, Ytrain, outputs,Hlist, batch,m)
              WBmatrix = updation(WBmatrix, grads, eta, batch)
          # eta = eta/np.sqrt(epochs+1)
          print(f'Cost at sample value {m} is {cost}')
          print(f'epoch no. {i+1} is complete')
      return WBmatrix


  def compute_accuracy(X_train, X_test, Y_train, Y_test, params,Hlist,batch):
      val, uselessHere = np.array(predict(X_train, params, Hlist,batch))
      acc = 0
      for k in range(batch):
          if (val.T[:,k] == YtrainNew.T[k]).all():
              acc +=1
      return acc/batch

  def predict(X, params, Hlist,size):
      ans = []

      values, outputs = feedf(params,X,Hlist,size,0)
      pre = outputs
      for m in range(size):
          predictions = np.zeros(10)
          predictions[np.argmax(pre[m])] = 1
          ans.append(list(predictions))
      
      return ans, pre

  def oneHot2labels(X, params, Hlist,size):
      labels = []
      a,Ypred = predict(X, params, Hlist,size)
      for i in range(np.shape(Ypred)[0]):
          labels.append(np.argmax(Ypred[i]))
      return labels






  # Computation starts from here
  layer_sizes = [784,50,50,10]
  # print(np.shape(YtrainNew))
  # print(np.shape(batc(YtrainNew.T,100)))
  import time

  a = time.time()
  params = model(Xtrain, YtrainNew, layer_sizes, 4, .6,1)
  b  = time.time()
  print(f'time taken to train is: {b-a} seconds')



  predicted = oneHot2labels(Xtrain, params, layer_sizes,60000)
  # np.shape(Ytrain)

  confusion_matrix = metrics.confusion_matrix(Ytrain, predicted)
  accuracy = metrics.accuracy_score(Ytrain, predicted)

  with open(output, "a") as f:
    print(f'The train accuracy of the model is {accuracy}',file = f)


  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

  cm_display.plot()
  plt.show()



  predicted = oneHot2labels(Xtest, params, layer_sizes,10000)
  np.shape(Ytrain)

  confusion_matrix = metrics.confusion_matrix(Ytest, predicted)
  # accuracy = metrics.accuracy_score(Ytrain, predicted)
  accuracy = metrics.accuracy_score(Ytest, predicted)
  with open(output, "a") as f:
    print(f'The test accuracy of the model is {accuracy}',file = f)

  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

  cm_display.plot()
  plt.show()

  #****************************************************************************


  layer_sizes = [784,50,50,50,10]
  # print(np.shape(YtrainNew))
  # print(np.shape(batc(YtrainNew.T,100)))
  import time

  a = time.time()
  params = model(Xtrain, YtrainNew, layer_sizes, 4, .6,1)
  b  = time.time()
  print(f'time taken to train is: {b-a} seconds')


  predicted = oneHot2labels(Xtrain, params, layer_sizes,60000)
  # np.shape(Ytrain)

  confusion_matrix = metrics.confusion_matrix(Ytrain, predicted)
  accuracy = metrics.accuracy_score(Ytrain, predicted)

  with open(output, "a") as f:
    print(f'The train accuracy of the model is {accuracy}',file = f)


  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

  cm_display.plot()
  plt.show()



  predicted = oneHot2labels(Xtest, params, layer_sizes,10000)
  np.shape(Ytrain)

  confusion_matrix = metrics.confusion_matrix(Ytest, predicted)
  # accuracy = metrics.accuracy_score(Ytrain, predicted)
  accuracy = metrics.accuracy_score(Ytest, predicted)
  with open(output, "a") as f:
    print(f'The test accuracy of the model is {accuracy}',file = f)

  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

  cm_display.plot()
  plt.show()

  #********************************************************************************

  layer_sizes = [784,50,50,50,50,10]
  # print(np.shape(YtrainNew))
  # print(np.shape(batc(YtrainNew.T,100)))
  import time

  a = time.time()
  params = model(Xtrain, YtrainNew, layer_sizes, 4, .6,1)
  b  = time.time()
  print(f'time taken to train is: {b-a} seconds')





  predicted = oneHot2labels(Xtrain, params, layer_sizes,60000)
  # np.shape(Ytrain)

  confusion_matrix = metrics.confusion_matrix(Ytrain, predicted)
  accuracy = metrics.accuracy_score(Ytrain, predicted)

  with open(output, "a") as f:
    print(f'The train accuracy of the model is {accuracy}',file = f)


  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

  cm_display.plot()
  plt.show()



  predicted = oneHot2labels(Xtest, params, layer_sizes,10000)
  np.shape(Ytrain)

  confusion_matrix = metrics.confusion_matrix(Ytest, predicted)
  # accuracy = metrics.accuracy_score(Ytrain, predicted)
  accuracy = metrics.accuracy_score(Ytest, predicted)
  with open(output, "a") as f:
    print(f'The test accuracy of the model is {accuracy}',file = f)

  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

  cm_display.plot()
  plt.show()

  #****************************************************************************

  layer_sizes = [784,50,50,50,50,50,10]
  # print(np.shape(YtrainNew))
  # print(np.shape(batc(YtrainNew.T,100)))
  import time

  a = time.time()
  params = model(Xtrain, YtrainNew, layer_sizes, 3, .6,1)
  b  = time.time()
  print(f'time taken to train is: {b-a} seconds')



  predicted = oneHot2labels(Xtrain, params, layer_sizes,60000)
  # np.shape(Ytrain)

  confusion_matrix = metrics.confusion_matrix(Ytrain, predicted)
  accuracy = metrics.accuracy_score(Ytrain, predicted)

  with open(output, "a") as f:
    print(f'The train accuracy of the model is {accuracy}',file = f)


  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

  cm_display.plot()
  plt.show()



  predicted = oneHot2labels(Xtest, params, layer_sizes,10000)
  np.shape(Ytrain)

  confusion_matrix = metrics.confusion_matrix(Ytest, predicted)
  # accuracy = metrics.accuracy_score(Ytrain, predicted)
  accuracy = metrics.accuracy_score(Ytest, predicted)
  with open(output, "a") as f:
    print(f'The test accuracy of the model is {accuracy}',file = f)

  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

  cm_display.plot()
  plt.show()

"""***************************************************************************************************"""

if part =='g':
  
  from sklearn.neural_network import MLPClassifier
  from sklearn.datasets import make_classification
  hiddenL = (100,100)
  clf = MLPClassifier(hidden_layer_sizes = hiddenL).fit(Xtrain, Ytrain)
  clf.predict_proba(Xtest)

  clf.predict(Xtest)

  with open(output, "a") as f:
    print(f'The train accuracy of the model is {clf.score(Xtrain,Ytrain)}',file = f)
      print(f'The test accuracy of the model is {clf.score(Xtest, Ytest)}',file = f)


  confusion_matrix = metrics.confusion_matrix(Ytrain, clf.predict(Xtrain))
  # accuracy = metrics.accuracy_score(Ytrain, predicted)
  # accuracy = metrics.accuracy_score(Ytest, predicted)
  # print(f'The test accuracy of the model is {accuracy}')

  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

  cm_display.plot()
  plt.show()

  confusion_matrix = metrics.confusion_matrix(Ytest, clf.predict(Xtest))
  # accuracy = metrics.accuracy_score(Ytrain, predicted)
  # accuracy = metrics.accuracy_score(Ytest, predicted)
  # print(f'The test accuracy of the model is {accuracy}')

  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)

  cm_display.plot()
  plt.show()

"""G"""